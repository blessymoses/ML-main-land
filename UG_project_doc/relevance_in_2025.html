<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>From MEAD-Based Multi-Document Summarization to Today’s Data, ML & GenAI Stack</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root{
      --ink:#0f172a;--muted:#475569;--bg:#ffffff;--card:#f8fafc;--line:#e2e8f0;--brand:#0ea5e9
    }
    html,body{margin:0;padding:0;background:var(--bg);color:var(--ink);font:16px/1.6 system-ui,-apple-system,Segoe UI,Roboto,Inter,Arial}
    header{padding:48px 24px 16px;max-width:980px;margin:auto}
    h1{font-size:clamp(28px,4vw,40px);margin:0 0 8px}
    h2{font-size:clamp(20px,3vw,26px);margin:28px 0 8px}
    h3{font-size:20px;margin:24px 0 6px}
    p.lead{color:var(--muted);margin:0}
    main{max-width:980px;margin:8px auto 64px;padding:0 24px}
    .card{background:var(--card);border:1px solid var(--line);border-radius:16px;padding:20px;margin:16px 0}
    .grid{display:grid;gap:16px}
    .grid.cols-2{grid-template-columns:repeat(auto-fit,minmax(280px,1fr))}
    .tag{display:inline-block;background:#e0f2fe;color:#075985;border:1px solid #bae6fd;border-radius:999px;padding:2px 10px;font-size:12px;margin-right:6px}
    ul{margin:10px 0 0 18px}
    code, pre{background:#0b1220;color:#e5e7eb;border-radius:10px}
    code{padding:2px 6px}
    pre{padding:14px;overflow:auto}
    .pill{font-weight:600;color:#0369a1}
    .kpi{display:flex;flex-wrap:wrap;gap:16px}
    .kpi .item{flex:1 1 220px;border:1px dashed var(--line);border-radius:12px;padding:12px;background:#fff}
    footer{max-width:980px;margin:24px auto 48px;padding:0 24px;color:var(--muted);font-size:14px}
    a{color:var(--brand);text-decoration:none}
    .timeline{border-left:3px solid var(--line);margin-left:8px;padding-left:16px}
    .okrs{display:grid;gap:8px}
    .okrs .row{display:grid;grid-template-columns:160px 1fr;gap:12px}
  </style>
</head>
<body>
<header>
  <h1>From MEAD-Style Extraction to Agentic GenAI: A Tech Story Across a Decade</h1>
  <p class="lead">The path was charted from an undergraduate prototype to an enterprise-ready, agentic summarization capability. What started as frequency and position heuristics was carried forward into vector search, retrieval‑augmented generation, and evaluation‑driven operations.</p>
</header>

<main>
  <section class="card">
    <h2>Prologue (circa 2010): when a corpus learned to speak</h2>
    <p>In a campus lab, a multi‑document summarizer was assembled. It was decided that summaries would be generated not by paraphrasing, but by extracting sentences most likely to carry meaning. Documents were monitored for opens. A “frequently visited” subset was selected. Sentences were scored by term frequency, absolute sentence position, relative paragraph position, and recency. A target compression ratio was honored, and a crisp executive digest was produced.</p>
    <div class="kpi">
      <div class="item"><span class="pill">Signals</span><br/>Term frequency • position • timestamp</div>
      <div class="item"><span class="pill">Human loop</span><br/>Opens and visits as a proxy for demand</div>
      <div class="item"><span class="pill">Control</span><br/>Compression ratio to fit attention budgets</div>
    </div>
  </section>

  <section class="card">
    <h2>Chapter 1 — the ideas that refused to die</h2>
    <p>The core intuitions were preserved. Centroids were later echoed by vector embeddings. Frequency and position were later absorbed by learned rankers. The “frequently visited” heuristic was later generalized into feedback loops. The timestamp bias was later formalized as recency‑aware retrieval. None of it was thrown away; it was upgraded.</p>
    <div class="grid cols-2">
      <div class="card">
        <h3>Then</h3>
        <ul>
          <li>Bag‑of‑words signals with centroid flavor</li>
          <li>Rule‑based scoring and redundancy trimming</li>
          <li>Usage counts as a filter for important docs</li>
          <li>Fixed‑ratio output for predictable length</li>
        </ul>
      </div>
      <div class="card">
        <h3>Now</h3>
        <ul>
          <li>Embeddings + ANN vector stores (with metadata)</li>
          <li>Hybrid search (BM25 + k‑NN) with learned rerankers</li>
          <li>Policy‑driven biasing by demand &amp; permissions</li>
          <li>LLM synthesis with citations and structure control</li>
        </ul>
      </div>
    </div>
  </section>

  <section class="card">
    <h2>Chapter 2 — the stack that would be deployed today</h2>
    <div class="grid cols-2">
      <div>
        <h3>Data &amp; indexing</h3>
        <ul>
          <li>Documents are ingested via connectors (files, mail, wikis, tickets). OCR and PII scrubbing are applied.</li>
          <li>Chunks are embedded and stored with source, author, and timestamp. Snapshots are versioned.</li>
          <li>Hybrid retrieval is configured; a recency prior is maintained.</li>
        </ul>
      </div>
      <div>
        <h3>Generation &amp; control</h3>
        <ul>
          <li>RAG prompts are templated by audience (exec, eng, ops).</li>
          <li>Cross‑encoder rerankers are used to improve support coverage.</li>
          <li>Citations are enforced and unsupported spans are flagged.</li>
        </ul>
      </div>
    </div>
    <div class="grid cols-2">
      <div>
        <h3>Observability</h3>
        <ul>
          <li>Retrieval hit‑rate, support coverage, groundedness score</li>
          <li>Latency/throughput SLOs and cost per 1k tokens</li>
          <li>Drift monitors for embeddings and index freshness</li>
        </ul>
      </div>
      <div>
        <h3>Governance</h3>
        <ul>
          <li>Row‑level security inherited from sources</li>
          <li>Retention policies and right‑to‑forget pipelines</li>
          <li>CI gates for safety, toxicity, and PII leakage</li>
        </ul>
      </div>
    </div>
  </section>

  <section class="card">
    <h2>Chapter 3 — the agentic turn</h2>
    <p>As corpora grew and audiences diversified, a single prompt was found insufficient. A minimal cast of agents was introduced and orchestration was adopted.</p>
    <ul>
      <li><strong>Planner agent:</strong> It was asked to decompose the reporting task into retrieval → synthesis → critique → delivery.</li>
      <li><strong>Retriever agent:</strong> It was instructed to balance authority and recency using hybrid search and rerankers.</li>
      <li><strong>Synthesizer agent:</strong> It was constrained to cite, to obey length/structure, and to maintain domain terminology.</li>
      <li><strong>Verifier agent:</strong> It was tasked to run NLI‑style checks and spot hallucinations and missing support.</li>
      <li><strong>Feedback agent:</strong> It was told to digest edits and to update prompts, weights, and routing rules.</li>
    </ul>
  </section>

  <section class="card">
    <h2>Field notes — what changed, what stayed</h2>
    <div class="grid cols-2">
      <div>
        <h3>What changed</h3>
        <ul>
          <li>Heuristics gave way to learned retrieval and synthesis.</li>
          <li>Static outputs were replaced by audience‑aware templates.</li>
          <li>Implicit demand signals were expanded to rich telemetry.</li>
        </ul>
      </div>
      <div>
        <h3>What stayed</h3>
        <ul>
          <li>Selection before generation remained a winning pattern.</li>
          <li>Recency and salience remained central to usefulness.</li>
          <li>Determinism and auditability remained non‑negotiable.</li>
        </ul>
      </div>
    </div>
  </section>

  <section class="card">
    <h2>Architecture v2 — reference blueprint</h2>
    <div class="grid cols-2">
      <div>
        <h3>Flow</h3>
        <div class="timeline">
          <p><strong>Ingest</strong> → connectors, OCR, PII scrub</p>
          <p><strong>Index</strong> → embeddings, BM25, metadata</p>
          <p><strong>Retrieve</strong> → k‑NN + reranker (recency‑aware)</p>
          <p><strong>Synthesize</strong> → LLM with citations &amp; structure</p>
          <p><strong>Assess</strong> → groundedness, coverage, toxicity</p>
          <p><strong>Learn</strong> → telemetry‑driven tuning</p>
        </div>
      </div>
      <div>
        <h3>Delivery</h3>
        <ul>
          <li>Executive briefs, engineering roll‑ups, customer memos</li>
          <li>Multilingual outputs with controlled terminology</li>
          <li>Exporters: HTML, Markdown, PDF, and slide outlines</li>
        </ul>
      </div>
    </div>
  </section>

  <section class="card">
    <h2>Risks &amp; mitigations that must be planned</h2>
    <ul>
      <li><strong>Hallucinations:</strong> It is reduced by retrieval‑only answers, strict citation checks, and claim‑evidence validation.</li>
      <li><strong>Privacy leakage:</strong> It is mitigated by row‑level security, redaction, and differential access policies.</li>
      <li><strong>Freshness gaps:</strong> They are minimized through index TTLs, streaming updates, and decayed scoring.</li>
      <li><strong>Evaluation drift:</strong> It is contained by golden‑set regression tests and SLA alarms.</li>
    </ul>
  </section>

  <section class="card">
    <h2>Epilogue — why the early work still matters</h2>
    <p>The original system mattered because the right questions were asked: What is salient? What is timely? What does the audience need to see? Those questions were never deprecated. They were encoded into today’s retrieval, ranking, and agentic orchestration. The project was not a detour; it was a prototype of the present.</p>
  </section>
</main>

<footer>
  <p><strong>Provenance:</strong> This story was distilled from the original slides describing a MEAD‑style, multi‑document extraction system that selected frequently visited documents, scored sentences by frequency and position, biased by timestamps, and generated summaries under a target compression ratio.</p>
</footer>
</body>
</html>